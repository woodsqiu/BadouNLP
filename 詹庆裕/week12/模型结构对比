模型	        Attention方式	transformer层数	归一化机制	  Mlp            	moe	                    位置编码	激活函数	hidden_size	   归一化位置	      Attention实现
Deepseek（3）	Multi-head	     61          	RMSNorm	  LLama2/并行	      Share-expert（mlp路由）	  Rope	      silu	    7168	       Pre——LN	        hidden_state进入后先经过两个线性层，一个query，一个key+value；从query线性层出来后，维度由7168->128*192，形状为（B，128，L，192），接着对最后一维进行切分，分别切成hidden_dim为128，64的张量，对dim=64做选择位置编码，重新与dim=128合并；kv层方面，hidden_state经过key+value的线性层变成一个大张量，同样切分为两个张量，一个k的pe张量进行旋转位置编码（dim=64）,另外一个张量包括k_nope与v，经过归一化后再次切分为k_nope(128), v(128),k_nope与k_pe重新合并，并与q进行点积操作（计算token与token直接的关系），将计算后的结果施加在v上后再经过一个线性层转化为原来的形状（b，l，7168）
ChatGPT（2）	Multi-query	     28	          RMSNorm	  feed_forward/串行	 无	                      Rope	      silu	    4096	       Pre——LN	        隐藏层维度经过一个qkv的大矩阵后，将其切分为q（32*128），k（2*128）,v（2*128），然后将q，k分别应用相对位置编码，之后通过将k，v插入维度并扩展维度变成和q一样的形状，然后计算q，k每个token之间的联系后施加在v上并进行输出
Baichuan（2）	Multi-head	     32	          RMSNorm	  LLama2/并行	       无	                      Rope	      silu	    4096	       Pre——LN	        与chatgpt3一样，隐藏层维度进入一个大线性层，从hidden_size->3*hidden_size，然后将输出的张量切分成3等分，每份都是（B，L，32，128）,然后q，k，v分别过相对位置编码后，q，k应用自注意力机制并施加在v上，最后调整回原来的hidden_states形状输出
Qinwen（2）  	Multi-head	     32	          RMSNorm	  LLama2/并行	       无	                      Rope	      silu	    4096	       Pre——LN	        与chatgpt3一样，隐藏层维度进入一个大线性层，从hidden_size->3*hidden_size，然后将输出的张量切分成3等分，每份都是（B，L，32，128）,然后q，k分别过相对位置编码后，q，k应用自注意力机制并施加在v上，最后调整回原来的hidden_states形状输出
grok1（1）	  Milti-query	     64	          RMSNorm	  LLama2/并行	       moe（线性层路由）	      Rope	      silu	    6144	       Sandwitch———LN 	隐藏层维度分别经过q，k，v线性层q（48*128），k（8*128）,v（8*128），然后将q，k分别应用相对位置编码，之后通过将k，v插入维度并扩展维度变成和q一样的形状，然后计算q，k每个token之间的联系后施加在v上，调整回hidden_state的形状并进行输出
Moss（2）   	Multi-head	     28	          RMSNorm	  feed_forward/串行  无	                      Rope	      silu	    4096	       Pre——LN(简化)	  与chatgpt3一样，隐藏层维度进入一个大线性层，从hidden_size->3*hidden_size，然后将输出的张量切分成3等分，每份都是（B，L，16，256）,然后q，k分别过相对位置编码后，q，k应用自注意力机制并施加在v上，最后调整回原来的hidden_states形状输出
DBRX（1）	    Multi-query	     40	         LayerNorm	LLama2/并行	       moe（线性层路由）	      Rope	      silu	    6144	       Pre——LN	        隐藏层维度分别经过q，k，v线性层q（48*128），k（8*128）,v（8*128），然后将q，k分别应用相对位置编码，之后通过将k，v插入维度并扩展维度变成和q一样的形状，然后计算q，k每个token之间的联系后施加在v上，调整回hidden_state的形状并进行输出
Mixtral（1）	Multi-query	     32	          RMSNorm	  LLama2/并行	       moe（线性层路由）	      Rope	      silu	    4096	       Pre——LN	        隐藏层维度分别经过q，k，v线性层q（32*128），k（8*128）,v（8*128），然后将q，k分别应用相对位置编码，之后通过将k，v插入维度并扩展维度变成和q一样的形状，然后计算q，k每个token之间的联系后施加在v上，调整回hidden_state的形状并进行输出
gemma(1)     	Multi-head	     28	          RMSNorm	  LLama2/并行	       无）	                    Rope	      gelu	    3072	       Pre——LN	        隐藏层维度分别经过q，k，v线性层q（16*128），k（16*128）,v（16*128），然后将q，k分别应用相对位置编码，之后通过将k，v插入维度并扩展维度变成和q一样的形状，然后计算q，k每个token之间的联系后施加在v上，调整回hidden_state的形状并进行输出，实际上还是应用了mulit—head的机制，因为q，k，v块数相同
