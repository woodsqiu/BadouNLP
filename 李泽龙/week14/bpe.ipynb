{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (231, 154, 132) into a new token 256\n",
      "merging (239, 188, 140) into a new token 257\n",
      "merging (227, 128, 130) into a new token 258\n",
      "merging (228, 184, 128) into a new token 259\n",
      "merging (228, 187, 150) into a new token 260\n",
      "merging (229, 188, 185) into a new token 261\n",
      "merging (257, 154) into a new token 262\n",
      "merging (256, 230) into a new token 263\n",
      "merging (229, 144, 142) into a new token 264\n",
      "merging (232, 131, 189) into a new token 265\n",
      "merging (229, 135, 187) into a new token 266\n",
      "merging (230, 156, 186) into a new token 267\n",
      "merging (233, 163, 158) into a new token 268\n",
      "merging (229, 156, 168) into a new token 269\n",
      "merging (228, 186, 134) into a new token 270\n",
      "merging (229, 175, 188) into a new token 271\n",
      "merging (271, 261) into a new token 272\n",
      "merging (230, 138, 128) into a new token 273\n",
      "merging (273, 265) into a new token 274\n",
      "merging (229, 146, 140) into a new token 275\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#token转换\n",
    "# 指定要读取的文件路径\n",
    "file_path = 'E:/nlp_learn/practice/week14/dota2英雄介绍-byRAG/Heroes/矮人直升机.txt'\n",
    "\n",
    "try:\n",
    "    # 以只读模式打开文件，并指定编码为 UTF-8\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        # 读取文件的全部内容并赋值给变量 text\n",
    "        text = file.read()\n",
    "    #print(text)\n",
    "except FileNotFoundError:\n",
    "    print(f\"文件 {file_path} 未找到。\")\n",
    "except Exception as e:\n",
    "    print(f\"读取文件时出现错误: {e}\")\n",
    "    \n",
    "tokens = text.encode(\"utf-8\") # raw bytes\n",
    "tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 for convenience\n",
    "\n",
    "def is_chinese_start(byte):\n",
    "    return 0xE0 <= byte <= 0xF4\n",
    "  \n",
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if is_chinese_start(ids[i]):\n",
    "            # 处理汉字，将整个汉字作为一个单元\n",
    "            if i + 2 < len(ids) and is_chinese_start(ids[i]) and (0x80 <= ids[i + 1] <= 0xBF) and (\n",
    "                    0x80 <= ids[i + 2] <= 0xBF):\n",
    "                char = tuple(ids[i:i + 3])\n",
    "                counts[char] = counts.get(char, 0) + 1\n",
    "                i += 3\n",
    "                continue\n",
    "        if i < len(ids) - 1:\n",
    "            pair = (ids[i], ids[i + 1])\n",
    "            counts[pair] = counts.get(pair, 0) + 1\n",
    "        i += 1\n",
    "    return counts\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if isinstance(pair, tuple) and len(pair) == 3 and i < len(ids) - 2 and tuple(ids[i:i + 3]) == pair:\n",
    "            newids.append(idx)\n",
    "            i += 3\n",
    "        elif i < len(ids) - 1 and ids[i] == pair[0] and ids[i + 1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "\n",
    "# ---\n",
    "vocab_size = 276 # the desired final vocabulary size  超参数：预期的最终词表大小，根据实际情况自己设置，大的词表会需要大的embedding层\n",
    "num_merges = vocab_size - 256\n",
    "ids = list(tokens) # copy so we don't destroy the original list\n",
    "\n",
    "merges = {} # (int, int) -> int\n",
    "for i in range(num_merges):\n",
    "  stats = get_stats(ids)\n",
    "  pair = max(stats, key=stats.get)\n",
    "  idx = 256 + i\n",
    "  print(f\"merging {pair} into a new token {idx}\")\n",
    "  ids = merge(ids, pair, idx)\n",
    "  merges[pair] = idx\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length: 2433\n",
      "ids length: 1887\n",
      "compression ratio: 1.29X\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens length:\", len(tokens))\n",
    "print(\"ids length:\", len(ids))\n",
    "print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A P技能ogramm一他��Introduction to Uni了e\n"
     ]
    }
   ],
   "source": [
    "#给出一段编码输出文字转换\n",
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for pair, idx in merges.items():\n",
    "    if len(pair) == 2:\n",
    "        p0, p1 = pair\n",
    "        vocab[idx] = vocab[p0] + vocab[p1]\n",
    "    else:\n",
    "        vocab[idx] = bytes(pair)\n",
    "\n",
    "def decode(ids):\n",
    "  # given ids (list of integers), return Python string\n",
    "  tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "  text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "  return text\n",
    "\n",
    "print(decode([65, 32, 80, 274, 111, 103, 114, 97, 109, 109, 259, 260, 153, 250, 73, 110, 116, 114, 111, 100, 117, 99, 116, 105, 111, 110, 32, 116, 111, 32, 85, 110, 105, 270, 101,]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
